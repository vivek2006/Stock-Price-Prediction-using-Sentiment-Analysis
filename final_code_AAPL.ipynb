{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lFg5tqEkWoN",
        "outputId": "3f3d4995-3933-4569-e51b-05fff0068750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow Version: 2.12.0\n",
            "Num GPUs: 1\n",
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "import math # Mathematical functions \n",
        "import numpy as np # Fundamental package for scientific computing with Python\n",
        "import pandas as pd # Additional functions for analysing and manipulating data\n",
        "from datetime import date, timedelta, datetime # Date Functions\n",
        "from pandas.plotting import register_matplotlib_converters # This function adds plotting functions for calender dates\n",
        "import matplotlib.pyplot as plt # Important package for visualization - we use this to plot the market data\n",
        "import matplotlib.dates as mdates # Formatting dates\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error # Packages for measuring model performance / errors\n",
        "from tensorflow.keras import Sequential # Deep learning library, used for neural networks\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout # Deep learning classes for recurrent and regular densely-connected layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping # EarlyStopping during model training\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler # This Scaler removes the median and scales the data according to the quantile range to normalize the price data \n",
        "import seaborn as sns # Visualization\n",
        "sns.set_style('white', { 'axes.spines.right': False, 'axes.spines.top': False})\n",
        "\n",
        "# check the tensorflow version and the number of available GPUs\n",
        "print('Tensorflow Version: ' + tf.__version__)\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs:\", len(physical_devices))\n",
        "\n",
        "import yfinance as yf \n",
        "df = yf.download(\"AAPL\", start = \"2008-08-08\", end = \"2014-06-08\", interval = '1d')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first=[0.0, 0.2, -0.04, 0.16, 0.04, -0.08, 0.08, 0.24, 0.08, 0.04, 0.04, 0.12, 0.0, -0.16, 0.24, -0.08, 0.24, 0.08, 0.32, 0.12, 0.12, -0.16, 0.08, 0.08, 0.04, 0.12, 0.12, 0.12, 0.24, -0.04, 0.08, 0.04, 0.08, 0.12, 0.0, -0.08, 0.12, 0.2, 0.12, 0.0, -0.08, -0.12, 0.04, 0.12, -0.08, 0.32, -0.04, -0.16, -0.08, 0.08, 0.08, -0.2, -0.2, -0.08, -0.16, 0.0, 0.12, 0.0, -0.04, 0.0, -0.2, 0.0, -0.04, -0.12, 0.0, -0.08, -0.16, -0.16, -0.16, -0.2, 0.0, -0.04, 0.0, -0.04, -0.04, -0.2, -0.16, -0.04, -0.04, -0.12, 0.0, -0.04, -0.08, 0.0, -0.04, -0.2, 0.0, -0.16, -0.04, -0.04, 0.0, -0.08, -0.2, -0.12, -0.2, -0.08, 0.0, -0.08, -0.08, -0.16, -0.16, -0.12, -0.12, -0.12, -0.04, -0.12, -0.04, 0.0, -0.12, -0.16, -0.04, 0.0, -0.04, -0.04, -0.12, -0.04, 0.04, -0.08, -0.08, 0.04, 0.0, -0.04, 0.0, -0.04, -0.32, -0.04, -0.08, -0.04, -0.12, -0.04, 0.04, -0.16, 0.0, 0.12, -0.12, -0.16, 0.0, -0.12, -0.08, -0.08, -0.08, 0.0, -0.12, -0.2, -0.08, -0.08, -0.12, -0.12, -0.08, 0.04, -0.16, -0.08, -0.2, -0.04, -0.16, 0.08, -0.12, -0.2, -0.04, -0.2, -0.12, -0.08, 0.0, -0.16, 0.0, -0.2, -0.12, -0.08, 0.04, -0.16, -0.08, -0.08, -0.12, -0.12, -0.04, -0.04, 0.0, -0.04, -0.04, 0.08, -0.16, -0.04, -0.28, -0.04, -0.04, -0.08, -0.04, -0.04, -0.08, 0.0, -0.04, -0.16, -0.08, -0.12, 0.0, -0.08, -0.24, -0.24, -0.12, -0.2, 0.0, -0.04, -0.2, -0.08, 0.0, -0.04, -0.08, 0.0, -0.04, -0.08, 0.04, -0.04, -0.04, -0.16, 0.0, 0.08, -0.08, 0.04, -0.16, -0.2, -0.2, 0.0, 0.0, -0.2, -0.04, 0.0, -0.04, -0.28, -0.24, -0.12, -0.08, -0.08, -0.12, -0.08, -0.12, -0.08, -0.16, -0.12, -0.12, -0.12, -0.24, -0.08, 0.04, -0.16, -0.08, -0.04, -0.12, -0.12, -0.04, -0.08, -0.2, -0.12, -0.04, -0.16, -0.08, 0.08, -0.04, -0.12, 0.0, -0.04, -0.2, 0.0, -0.08, -0.12, 0.0, 0.04, -0.16, -0.12, -0.12, -0.12, -0.24, -0.08, 0.04, -0.16, -0.08, -0.04, -0.12, -0.12, -0.04, -0.08, -0.2, -0.12, -0.04, -0.16, -0.08, 0.08, -0.04, -0.12, 0.0, -0.04, -0.2, 0.0, -0.08, -0.12, 0.0, 0.04, -0.16, -0.12, -0.12, -0.12, 0.0, -0.2, 0.04, -0.04, -0.24, -0.08, -0.2, -0.16, -0.12, -0.2, -0.28, 0.0, -0.04, -0.12, -0.2, 0.0, -0.08, -0.2, -0.04, -0.16, 0.08, -0.04, -0.08, -0.08, -0.08, -0.08, -0.16, -0.2, -0.16, -0.2, -0.12, -0.32, -0.08, -0.08, -0.12, -0.24, -0.2, -0.08, -0.08, 0.0, -0.08, -0.28, 0.08, -0.08, -0.28, -0.16, -0.2, 0.04, -0.04, 0.0, -0.04, -0.2, -0.2, -0.08, -0.16, -0.08, 0.08, -0.28, -0.08, -0.12]\n"
      ],
      "metadata": {
        "id": "J2KQG78hljwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "four_years= [-0.28, 0.0, -0.24, -0.16, 0.0, -0.12, -0.12, 0.0, -0.12, -0.2, -0.16, -0.08, -0.08, -0.04, 0.08, -0.24, -0.2, -0.08, -0.16, -0.08, -0.08, -0.24, -0.24, -0.16, -0.12, -0.24, -0.24, -0.2, -0.16, -0.2, -0.24, -0.16, -0.32, -0.2, -0.12, -0.12, -0.2, -0.16, -0.08, -0.08, -0.24, -0.16, -0.16, -0.16, -0.28, -0.2, -0.16, -0.12, -0.24, 0.0, 0.0, -0.16, -0.04, -0.12, -0.52, -0.2, -0.2, -0.24, -0.2, -0.24, -0.08, -0.2, -0.12, -0.28, -0.12, 0.04, -0.08, -0.28, -0.08, -0.12, -0.08, -0.2, 0.08, -0.12, -0.2, -0.24, -0.16, -0.12, -0.08, -0.12, 0.04, -0.12, -0.08, -0.12, -0.08, -0.28, -0.28, -0.2, -0.12, -0.04, -0.08, 0.0, -0.2, -0.2, -0.36, -0.08, -0.04, -0.12, -0.16, -0.24, -0.16, -0.04, -0.04, -0.2, -0.2, -0.08, -0.08, -0.08, -0.12, -0.28, -0.12, -0.12, -0.16, -0.12, -0.16, -0.16, -0.12, -0.04, -0.2, -0.04, -0.16, -0.08, -0.16, -0.16, -0.2, -0.04, -0.08, -0.08, -0.2, -0.08, -0.12, -0.2, -0.24, -0.16, -0.16, -0.16, -0.08, -0.16, -0.12, -0.04, 0.0, -0.12, -0.12, -0.2, 0.0, -0.12, -0.04, -0.12, -0.16, -0.12, -0.2, -0.16, -0.12, -0.08, -0.08, -0.16, -0.2, -0.2, -0.16, -0.12, -0.24, -0.16, -0.12, -0.12, -0.2, -0.24, -0.08, -0.08, -0.16, -0.12, -0.2, -0.12, 0.12, -0.12, -0.2, -0.04, 0.04, -0.04, -0.08, 0.04, -0.24, -0.08, -0.28, -0.08, -0.04, -0.2, -0.04, -0.08, -0.12, -0.08, 0.0, -0.08, -0.2, 0.04, -0.08, -0.08, -0.2, -0.28, -0.16, -0.12, -0.24, -0.08, -0.2, -0.12, -0.12, -0.32, -0.2, -0.04, -0.08, -0.2, -0.08, -0.08, 0.04, -0.2, -0.12, -0.24, -0.24, 0.0, -0.12, -0.4, -0.2, 0.0, -0.2, -0.2, -0.16, -0.16, -0.04, -0.24, -0.08, -0.12, -0.2, -0.24, -0.08, -0.28, -0.08, -0.12, -0.16, -0.24, -0.28, 0.0, -0.2, -0.2, -0.24, -0.16, -0.24, -0.16, -0.36, -0.16, -0.2, 0.04, -0.16, -0.12, -0.28, -0.04, 0.04, -0.12, 0.0, -0.16, -0.24, -0.08, -0.04, -0.28, -0.08, -0.16, -0.08, -0.12, -0.16, 0.0, -0.24, -0.04, -0.2, -0.2, -0.04, -0.04, -0.08, -0.12, -0.2, -0.08, -0.08, -0.12, -0.28, -0.24, -0.2, -0.08, -0.08, -0.08, -0.12, -0.04, -0.24, -0.12, -0.16, -0.24, -0.12, -0.16, -0.08, -0.12, -0.04, -0.28, -0.24, -0.32, -0.08, -0.24, 0.04, 0.04, -0.32, -0.16, -0.08, -0.08, -0.24, -0.32, -0.16, 0.0, -0.24, -0.04, -0.16, -0.2, -0.08, -0.2, -0.04, -0.32, -0.2, -0.16, -0.32, 0.0, -0.24, 0.04, -0.2, -0.16, -0.2, -0.08, -0.12, -0.24, -0.12, -0.12, -0.32, -0.28, -0.04, 0.0, -0.24, -0.16, -0.04, -0.24, -0.12, -0.32, -0.2, 0.0, -0.28, -0.04, -0.04, -0.08, -0.16, -0.16, -0.12, -0.08, -0.24, -0.12, -0.16, -0.2, -0.04, -0.28, -0.2, 0.04, -0.16, 0.08, -0.12, -0.16, -0.04, -0.16, -0.16, -0.12, -0.24, -0.2, 0.0, -0.04, -0.36, -0.16, -0.2, -0.28, -0.16, -0.36, 0.0, -0.2, -0.12, 0.0, -0.12, -0.24, -0.08, -0.08, -0.12, -0.2, -0.2, -0.08, -0.16, -0.08, 0.04, -0.2, -0.04, -0.12, -0.24, -0.12, -0.08, -0.08, -0.04, -0.12, 0.0, 0.0, -0.2, -0.2, 0.0, 0.04, -0.36, -0.08, -0.04, -0.12, -0.12, -0.08, -0.24, -0.04, -0.12, -0.12, -0.08, -0.08, -0.12, -0.24, -0.12, -0.12, -0.2, -0.08, 0.04, -0.16, -0.16, -0.2, -0.16, -0.24, 0.0, -0.04, -0.24, -0.16, 0.0, -0.08, -0.16, -0.24, -0.16, -0.32, 0.0, -0.16, -0.2, -0.12, -0.12, -0.12, -0.12, -0.16, -0.12, -0.08, -0.04, -0.36, -0.08, -0.2, -0.2, -0.24, -0.08, -0.24, -0.2, -0.08, -0.12, -0.28, -0.4, -0.2, -0.16, -0.16, -0.12, -0.28, -0.2, -0.16, -0.04, 0.04, -0.12, -0.48, -0.2, -0.2, -0.04, -0.24, -0.4, -0.2, 0.0, -0.12, -0.16, -0.08, 0.04, -0.12, -0.12, -0.2, 0.0, -0.2, -0.16, -0.16, -0.28, -0.32, -0.2, -0.08, -0.28, -0.2, -0.16, -0.28, -0.24, -0.24, -0.12, -0.2, -0.08, -0.16, -0.12, -0.16, -0.2, -0.4, -0.2, -0.24, -0.36, -0.16, -0.24, -0.2, -0.36, -0.04, -0.2, -0.2, -0.24, -0.28, -0.16, -0.04, -0.04, -0.12, -0.12, -0.04, -0.32, -0.28, -0.2, -0.2, -0.16, -0.44, -0.2, -0.2, -0.08, -0.24, -0.36, -0.08, -0.08, -0.04, -0.24, -0.32, -0.24, -0.08, -0.08, -0.32, -0.24, -0.16, -0.12, -0.32, -0.4, -0.32, -0.28, -0.24, -0.24, -0.04, -0.2, -0.4, -0.12, -0.28, -0.36, -0.2, -0.2, -0.12, -0.24, -0.32, -0.16, 0.04, -0.28, -0.24, -0.16, -0.2, -0.2, -0.32, -0.32, -0.24, -0.12, -0.08, -0.2, -0.24, 0.0, -0.12, -0.16, -0.2, -0.24, -0.28, -0.28, -0.4, -0.2, -0.12, -0.12, -0.04, -0.04, -0.16, -0.12, -0.08, -0.08, -0.16, -0.04, -0.16, -0.2, -0.44, -0.16, -0.04, -0.2, -0.04, -0.08, -0.08, -0.12, -0.24, -0.16, -0.08, -0.12, -0.32, -0.2, -0.16, -0.16, -0.24, -0.16, -0.04, -0.32, -0.12, -0.2, -0.08, -0.2, -0.24, -0.2, -0.12, -0.4, -0.2, -0.28, -0.24, -0.16, -0.16, -0.16, -0.04, -0.04, -0.08, -0.28, -0.16, -0.2, -0.08, -0.24, -0.24, -0.2, -0.28, -0.28, -0.12, -0.4, -0.28, -0.04, -0.28, -0.16, -0.2, -0.32, -0.2, -0.28, -0.2, -0.24, -0.08, -0.12, -0.28, -0.08, -0.28, -0.24, -0.08, -0.2, -0.24, -0.12, -0.16, -0.24, -0.04, -0.2, -0.16, -0.08, -0.24, -0.24, -0.08, -0.32, -0.2, -0.08, -0.24, -0.16, -0.24, -0.16, 0.0, -0.16, -0.16, -0.32, -0.16, -0.16, -0.12, -0.36, -0.36, -0.32, -0.2, -0.28, -0.2, -0.16, -0.16, -0.2, -0.36, -0.32, -0.24, -0.16, -0.16, -0.2, -0.2, -0.28, -0.2, -0.4, -0.32, -0.2, -0.32, -0.24, -0.32, -0.12, -0.32, -0.12, -0.16, -0.24, -0.32, -0.04, 0.0, -0.36, -0.2, -0.2, -0.32, -0.36, -0.4, -0.12, -0.08, -0.24, -0.24, -0.16, -0.12, -0.16, -0.2, -0.16, -0.16, -0.04, -0.24, -0.16, -0.4, -0.36, -0.24, -0.24, -0.32, -0.36, -0.2, -0.24, -0.04, -0.24, -0.16, -0.08, -0.36, -0.16, -0.32, -0.2, -0.16, -0.16, -0.12, -0.36, -0.28, -0.32, -0.28, -0.16, -0.2, -0.24, -0.16, -0.48, -0.24, -0.2, -0.36, -0.2, -0.32, -0.12, -0.4, -0.16, -0.32, -0.24, -0.28, -0.2, -0.16, -0.08, -0.32, -0.08, -0.28, -0.16, -0.2, -0.28, -0.2, -0.4, -0.04, -0.12, -0.4, -0.48, -0.12, -0.4, -0.08, -0.28, -0.04, -0.08, -0.16, -0.24, -0.28, -0.2, -0.12, -0.16, -0.2, -0.12, -0.08, -0.32, -0.24, -0.2, -0.12, -0.36, -0.28, -0.2, -0.32, -0.24, -0.12, -0.32, -0.28, -0.2, -0.28, -0.16, -0.16, -0.28, -0.24, -0.32, -0.36, -0.2, -0.28, -0.2, -0.28, -0.2, -0.32, -0.32, -0.32, -0.44, -0.4, -0.2, -0.2, -0.36, -0.2, -0.32, -0.2, -0.2, -0.32, -0.32, -0.24, -0.32, -0.28, -0.2, -0.04, -0.4, -0.2, -0.08, 0.04, -0.28, -0.12, -0.12, -0.36, -0.24, -0.36, -0.4, -0.28, -0.28, -0.24, -0.36, -0.24, -0.28, -0.28, -0.36, -0.08, -0.2, -0.08, -0.32, -0.2, -0.2, -0.28, -0.36, -0.4, -0.24, -0.32, -0.16, -0.2, -0.08, -0.2, -0.16, -0.12, -0.24, -0.24, -0.28, -0.12, -0.4, -0.28, -0.2, -0.16, -0.28, -0.16, -0.08, -0.24, -0.12, -0.16, -0.2, -0.12, -0.16, -0.08, -0.2, -0.2, -0.16, -0.16, -0.4, -0.2, -0.24, 0.04, -0.24, -0.2, -0.28, -0.32, -0.16, -0.08, -0.32, -0.12, -0.16, -0.28, -0.2, -0.08, -0.24, -0.28, -0.2, -0.2, -0.2, -0.2, -0.16, -0.32, -0.16, -0.24, -0.2, -0.24, -0.28, -0.12, -0.24, -0.28, -0.36, -0.28, -0.12, -0.2, -0.24, -0.16, -0.16, -0.28, -0.28, -0.16, -0.16, -0.16, -0.16, -0.2, -0.32, -0.16, -0.16, -0.04, -0.08, -0.08, -0.12, -0.16, -0.12, -0.28, -0.16, -0.36, -0.28, -0.24, -0.28, -0.4, -0.2, -0.16, -0.12, -0.32, -0.24, -0.28, -0.28, -0.28, -0.12, -0.12, -0.2, -0.2, -0.16, -0.24, -0.2, -0.08, -0.36, -0.12, -0.12, -0.36, -0.2, 0.0, -0.12, -0.28, -0.2, -0.16, -0.36, -0.2, -0.12, -0.32, -0.28, -0.32, -0.4, -0.2, 0.08, -0.24, -0.2, -0.16, -0.28, -0.2, -0.24, -0.08, -0.28, -0.2, -0.2, -0.12, -0.28, -0.16, -0.28, -0.2, -0.32, -0.2, -0.2, -0.12, -0.24, -0.36, -0.32, -0.24, -0.24, -0.24, 0.0, 0.0, -0.44, -0.32, -0.2, -0.2, -0.08, -0.28, -0.12, -0.12, -0.24, -0.04, -0.12, -0.28, -0.24, -0.16, -0.12, -0.2, -0.04, -0.08, -0.08, -0.12, -0.36, -0.24, 0.0, -0.28, -0.04, -0.04, -0.44, -0.04, -0.32, -0.24, -0.16, -0.24, -0.32, -0.28, -0.24, -0.12, -0.2, -0.28, -0.12, -0.04, -0.12, -0.24, -0.32, -0.08, -0.44, -0.32, -0.24, -0.2, -0.12, -0.08, -0.24, -0.16, -0.08, -0.12, -0.12, -0.04, -0.16, -0.36, -0.2, -0.08, -0.16, -0.2, -0.24, -0.12, -0.36, 0.0, -0.08, -0.24, -0.2, -0.2, 0.0, -0.12, -0.16, 0.04, -0.28, -0.24, -0.2, -0.16, -0.16, -0.44, -0.08, -0.12, -0.16, -0.24, -0.2, -0.2, -0.16, -0.12, -0.2, -0.04, -0.32, -0.24, -0.12, -0.12, -0.16, -0.16, -0.28, -0.32, -0.16, -0.12, -0.24, -0.2, -0.2, 0.04, -0.44, -0.16, -0.16, -0.08, -0.24, -0.12, -0.08, 0.04, -0.24, -0.08, -0.24, -0.16, -0.4, -0.04, -0.12, -0.24, -0.2, -0.04, -0.08, -0.24, -0.24, -0.12, -0.2, -0.2, -0.12, 0.0, -0.12, 0.04, -0.16, -0.36, -0.12, -0.04, -0.28, -0.2, -0.2, -0.2, -0.44, -0.2, -0.28, -0.36, -0.16, -0.12, -0.16, -0.08, -0.24, -0.12, -0.2, -0.28, -0.32, -0.08, -0.24, -0.12, -0.16, -0.08, -0.04, -0.04, -0.28, -0.24, -0.16, -0.2, -0.2, -0.16, -0.36, -0.28, -0.32, -0.16, -0.12, -0.08, -0.12, -0.2, -0.2, -0.04, -0.2, -0.2, -0.24, -0.2, -0.24, -0.36, -0.24, -0.12, -0.12, -0.24, -0.24, -0.32, -0.36, -0.2, -0.16, -0.28, -0.2, -0.08, -0.08, -0.2, -0.24, -0.16, -0.36, -0.04, -0.24, -0.32, -0.4, -0.12, -0.08, -0.08, -0.36, -0.24, -0.16, -0.32, -0.28, -0.16, -0.28, -0.08, -0.24, -0.2, -0.08, -0.12, -0.32, -0.2, -0.16, -0.16, -0.32, -0.28, -0.28, -0.2, -0.2, 0.0, -0.16, -0.36, -0.16, -0.08, -0.16, -0.16, -0.2, -0.16, -0.12, 0.0, -0.08, -0.16, -0.2, -0.2, -0.16, -0.24, -0.16, -0.36, -0.12, -0.24, -0.08, -0.08, -0.24, -0.2, -0.2, -0.24, -0.08, -0.16, -0.08, -0.24, -0.32, -0.12, -0.04, -0.12, -0.16, -0.32, -0.2, -0.24, -0.28, -0.16, -0.2, -0.16, -0.28, -0.2, -0.24, -0.2, -0.12, -0.32, -0.16, -0.2, -0.32, -0.2, -0.2, -0.12, -0.04, -0.16, -0.2, -0.24, 0.0, -0.16, -0.28, -0.48, -0.2, -0.32, -0.08, -0.36, -0.08, -0.16, -0.16, -0.08, -0.2, -0.08, -0.16, -0.24, -0.12, -0.08, -0.08, -0.16, 0.08, -0.2, -0.28, -0.08, -0.2, -0.2, -0.2, 0.04, -0.16, 0.04, -0.28, -0.08, -0.12, -0.16, -0.28, -0.36, -0.08, 0.04, -0.16, -0.24, -0.04, -0.32, -0.24, 0.0, -0.16, -0.12, -0.36, -0.16, -0.2, -0.36, -0.08, -0.12, -0.24, -0.32, -0.2, -0.16, -0.2, -0.12, -0.12, -0.2, -0.16, -0.28, -0.24, 0.0, -0.12, -0.24, -0.04, -0.2, -0.24, -0.16, -0.2, -0.16, -0.28, -0.12, -0.16, -0.08, 0.0, -0.24, -0.32, -0.12, -0.16, -0.28, -0.2, -0.16, -0.2, -0.32, -0.36, -0.08, -0.08, -0.2, -0.28, 0.0, -0.16, -0.32, -0.12, -0.2, -0.2, -0.24, -0.16, -0.12, -0.16, -0.32, -0.32, -0.16, -0.16, -0.4, -0.36, -0.16, -0.2, -0.12]"
      ],
      "metadata": {
        "id": "ODMQ-xP0he9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv(\"/content/upload_DJIA_table.csv\")"
      ],
      "metadata": {
        "id": "nFyOrIsPllXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = df.loc[::-1].reset_index(drop=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "N5DcB5PamCNO",
        "outputId": "39a84068-5968-4afa-f59e-e6baacff390f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Open       High        Low      Close  Adj Close     Volume\n",
              "Date                                                                        \n",
              "2008-08-08   5.852143   6.058929   5.848214   6.055357   5.153826  713997200\n",
              "2008-08-11   6.073929   6.303571   6.059643   6.198571   5.275718  891304400\n",
              "2008-08-12   6.197143   6.403214   6.196786   6.311786   5.372077  836278800\n",
              "2008-08-13   6.356429   6.428571   6.282143   6.403571   5.450197  842346400\n",
              "2008-08-14   6.368929   6.444643   6.351429   6.404286   5.450805  711300800\n",
              "...               ...        ...        ...        ...        ...        ...\n",
              "2014-06-02  22.641430  22.672501  22.232143  22.451786  19.992477  369350800\n",
              "2014-06-03  22.445000  22.812143  22.437500  22.769285  20.275200  292709200\n",
              "2014-06-04  22.765715  23.138929  22.718214  23.029285  20.506723  335482000\n",
              "2014-06-05  23.078571  23.191786  22.950357  23.119642  20.587179  303805600\n",
              "2014-06-06  23.210714  23.259287  23.016787  23.056070  20.530573  349938400\n",
              "\n",
              "[1467 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d7484d98-350e-4640-a0cf-3b123cda7e6c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>5.852143</td>\n",
              "      <td>6.058929</td>\n",
              "      <td>5.848214</td>\n",
              "      <td>6.055357</td>\n",
              "      <td>5.153826</td>\n",
              "      <td>713997200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-11</th>\n",
              "      <td>6.073929</td>\n",
              "      <td>6.303571</td>\n",
              "      <td>6.059643</td>\n",
              "      <td>6.198571</td>\n",
              "      <td>5.275718</td>\n",
              "      <td>891304400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-12</th>\n",
              "      <td>6.197143</td>\n",
              "      <td>6.403214</td>\n",
              "      <td>6.196786</td>\n",
              "      <td>6.311786</td>\n",
              "      <td>5.372077</td>\n",
              "      <td>836278800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-13</th>\n",
              "      <td>6.356429</td>\n",
              "      <td>6.428571</td>\n",
              "      <td>6.282143</td>\n",
              "      <td>6.403571</td>\n",
              "      <td>5.450197</td>\n",
              "      <td>842346400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-14</th>\n",
              "      <td>6.368929</td>\n",
              "      <td>6.444643</td>\n",
              "      <td>6.351429</td>\n",
              "      <td>6.404286</td>\n",
              "      <td>5.450805</td>\n",
              "      <td>711300800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-06-02</th>\n",
              "      <td>22.641430</td>\n",
              "      <td>22.672501</td>\n",
              "      <td>22.232143</td>\n",
              "      <td>22.451786</td>\n",
              "      <td>19.992477</td>\n",
              "      <td>369350800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-06-03</th>\n",
              "      <td>22.445000</td>\n",
              "      <td>22.812143</td>\n",
              "      <td>22.437500</td>\n",
              "      <td>22.769285</td>\n",
              "      <td>20.275200</td>\n",
              "      <td>292709200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-06-04</th>\n",
              "      <td>22.765715</td>\n",
              "      <td>23.138929</td>\n",
              "      <td>22.718214</td>\n",
              "      <td>23.029285</td>\n",
              "      <td>20.506723</td>\n",
              "      <td>335482000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-06-05</th>\n",
              "      <td>23.078571</td>\n",
              "      <td>23.191786</td>\n",
              "      <td>22.950357</td>\n",
              "      <td>23.119642</td>\n",
              "      <td>20.587179</td>\n",
              "      <td>303805600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-06-06</th>\n",
              "      <td>23.210714</td>\n",
              "      <td>23.259287</td>\n",
              "      <td>23.016787</td>\n",
              "      <td>23.056070</td>\n",
              "      <td>20.530573</td>\n",
              "      <td>349938400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1467 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7484d98-350e-4640-a0cf-3b123cda7e6c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d7484d98-350e-4640-a0cf-3b123cda7e6c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d7484d98-350e-4640-a0cf-3b123cda7e6c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[:-27]"
      ],
      "metadata": {
        "id": "iEdRkBLGmTRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentiment Score'] = four_years"
      ],
      "metadata": {
        "id": "Td-JnR3rnFwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d834698-eb80-42cf-9131-d48776263d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-07084179a58a>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Sentiment Score'] = four_years\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "qRUxoOB9nK0c",
        "outputId": "353299a6-370c-4b1d-863b-52d04bc49396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Open       High        Low      Close  Adj Close     Volume  \\\n",
              "Date                                                                           \n",
              "2008-08-08   5.852143   6.058929   5.848214   6.055357   5.153826  713997200   \n",
              "2008-08-11   6.073929   6.303571   6.059643   6.198571   5.275718  891304400   \n",
              "2008-08-12   6.197143   6.403214   6.196786   6.311786   5.372077  836278800   \n",
              "2008-08-13   6.356429   6.428571   6.282143   6.403571   5.450197  842346400   \n",
              "2008-08-14   6.368929   6.444643   6.351429   6.404286   5.450805  711300800   \n",
              "...               ...        ...        ...        ...        ...        ...   \n",
              "2014-04-23  18.895000  18.968929  18.730356  18.741072  16.595537  394940000   \n",
              "2014-04-24  20.293215  20.357143  20.026072  20.277500  17.956064  759911600   \n",
              "2014-04-25  20.161785  20.428213  20.141430  20.426430  18.087946  390275200   \n",
              "2014-04-28  20.457144  21.276787  20.448214  21.217501  18.788452  669485600   \n",
              "2014-04-29  21.205000  21.285000  21.053928  21.154642  18.732790  337377600   \n",
              "\n",
              "            Sentiment Score  \n",
              "Date                         \n",
              "2008-08-08            -0.28  \n",
              "2008-08-11             0.00  \n",
              "2008-08-12            -0.24  \n",
              "2008-08-13            -0.16  \n",
              "2008-08-14             0.00  \n",
              "...                     ...  \n",
              "2014-04-23            -0.40  \n",
              "2014-04-24            -0.36  \n",
              "2014-04-25            -0.16  \n",
              "2014-04-28            -0.20  \n",
              "2014-04-29            -0.12  \n",
              "\n",
              "[1440 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e735b5f-f62c-4eb1-9a50-1896b62a6db2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Sentiment Score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>5.852143</td>\n",
              "      <td>6.058929</td>\n",
              "      <td>5.848214</td>\n",
              "      <td>6.055357</td>\n",
              "      <td>5.153826</td>\n",
              "      <td>713997200</td>\n",
              "      <td>-0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-11</th>\n",
              "      <td>6.073929</td>\n",
              "      <td>6.303571</td>\n",
              "      <td>6.059643</td>\n",
              "      <td>6.198571</td>\n",
              "      <td>5.275718</td>\n",
              "      <td>891304400</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-12</th>\n",
              "      <td>6.197143</td>\n",
              "      <td>6.403214</td>\n",
              "      <td>6.196786</td>\n",
              "      <td>6.311786</td>\n",
              "      <td>5.372077</td>\n",
              "      <td>836278800</td>\n",
              "      <td>-0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-13</th>\n",
              "      <td>6.356429</td>\n",
              "      <td>6.428571</td>\n",
              "      <td>6.282143</td>\n",
              "      <td>6.403571</td>\n",
              "      <td>5.450197</td>\n",
              "      <td>842346400</td>\n",
              "      <td>-0.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-14</th>\n",
              "      <td>6.368929</td>\n",
              "      <td>6.444643</td>\n",
              "      <td>6.351429</td>\n",
              "      <td>6.404286</td>\n",
              "      <td>5.450805</td>\n",
              "      <td>711300800</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-04-23</th>\n",
              "      <td>18.895000</td>\n",
              "      <td>18.968929</td>\n",
              "      <td>18.730356</td>\n",
              "      <td>18.741072</td>\n",
              "      <td>16.595537</td>\n",
              "      <td>394940000</td>\n",
              "      <td>-0.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-04-24</th>\n",
              "      <td>20.293215</td>\n",
              "      <td>20.357143</td>\n",
              "      <td>20.026072</td>\n",
              "      <td>20.277500</td>\n",
              "      <td>17.956064</td>\n",
              "      <td>759911600</td>\n",
              "      <td>-0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-04-25</th>\n",
              "      <td>20.161785</td>\n",
              "      <td>20.428213</td>\n",
              "      <td>20.141430</td>\n",
              "      <td>20.426430</td>\n",
              "      <td>18.087946</td>\n",
              "      <td>390275200</td>\n",
              "      <td>-0.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-04-28</th>\n",
              "      <td>20.457144</td>\n",
              "      <td>21.276787</td>\n",
              "      <td>20.448214</td>\n",
              "      <td>21.217501</td>\n",
              "      <td>18.788452</td>\n",
              "      <td>669485600</td>\n",
              "      <td>-0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-04-29</th>\n",
              "      <td>21.205000</td>\n",
              "      <td>21.285000</td>\n",
              "      <td>21.053928</td>\n",
              "      <td>21.154642</td>\n",
              "      <td>18.732790</td>\n",
              "      <td>337377600</td>\n",
              "      <td>-0.12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1440 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e735b5f-f62c-4eb1-9a50-1896b62a6db2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e735b5f-f62c-4eb1-9a50-1896b62a6db2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e735b5f-f62c-4eb1-9a50-1896b62a6db2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing Batches\n",
        "train_df = df.sort_values(by=['Date']).copy()\n",
        "\n",
        "# List of considered Features\n",
        "FEATURES = ['High', 'Low', 'Open', 'Close', 'Volume', #'Sentiment Score'\n",
        "            #, 'Month', 'Year', 'Adj Close'\n",
        "           ]\n",
        "\n",
        "print('FEATURE LIST')\n",
        "print([f for f in FEATURES])\n",
        "\n",
        "data = pd.DataFrame(train_df)\n",
        "data_filtered = data[FEATURES]\n",
        "\n",
        "# We add a prediction column and set dummy values to prepare the data for scaling\n",
        "data_filtered_ext = data_filtered.copy()\n",
        "data_filtered_ext['Prediction'] = data_filtered_ext['Close']\n",
        "\n",
        "# Print the tail of the dataframe\n",
        "data_filtered_ext.tail()"
      ],
      "metadata": {
        "id": "4ONnwX-wnPyi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "54b99cdd-ef33-49f1-cc7a-464d2c2f0084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FEATURE LIST\n",
            "['High', 'Low', 'Open', 'Close', 'Volume']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 High        Low       Open      Close     Volume  Prediction\n",
              "Date                                                                         \n",
              "2014-04-23  18.968929  18.730356  18.895000  18.741072  394940000   18.741072\n",
              "2014-04-24  20.357143  20.026072  20.293215  20.277500  759911600   20.277500\n",
              "2014-04-25  20.428213  20.141430  20.161785  20.426430  390275200   20.426430\n",
              "2014-04-28  21.276787  20.448214  20.457144  21.217501  669485600   21.217501\n",
              "2014-04-29  21.285000  21.053928  21.205000  21.154642  337377600   21.154642"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7959e6f4-1177-4e9d-a0cb-92bff1a90366\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Prediction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2014-04-23</th>\n",
              "      <td>18.968929</td>\n",
              "      <td>18.730356</td>\n",
              "      <td>18.895000</td>\n",
              "      <td>18.741072</td>\n",
              "      <td>394940000</td>\n",
              "      <td>18.741072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-04-24</th>\n",
              "      <td>20.357143</td>\n",
              "      <td>20.026072</td>\n",
              "      <td>20.293215</td>\n",
              "      <td>20.277500</td>\n",
              "      <td>759911600</td>\n",
              "      <td>20.277500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-04-25</th>\n",
              "      <td>20.428213</td>\n",
              "      <td>20.141430</td>\n",
              "      <td>20.161785</td>\n",
              "      <td>20.426430</td>\n",
              "      <td>390275200</td>\n",
              "      <td>20.426430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-04-28</th>\n",
              "      <td>21.276787</td>\n",
              "      <td>20.448214</td>\n",
              "      <td>20.457144</td>\n",
              "      <td>21.217501</td>\n",
              "      <td>669485600</td>\n",
              "      <td>21.217501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-04-29</th>\n",
              "      <td>21.285000</td>\n",
              "      <td>21.053928</td>\n",
              "      <td>21.205000</td>\n",
              "      <td>21.154642</td>\n",
              "      <td>337377600</td>\n",
              "      <td>21.154642</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7959e6f4-1177-4e9d-a0cb-92bff1a90366')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7959e6f4-1177-4e9d-a0cb-92bff1a90366 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7959e6f4-1177-4e9d-a0cb-92bff1a90366');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of rows in the data\n",
        "nrows = data_filtered.shape[0]\n",
        "\n",
        "# Convert the data to numpy values\n",
        "np_data_unscaled = np.array(data_filtered)\n",
        "np_data = np.reshape(np_data_unscaled, (nrows, -1))\n",
        "print(np_data.shape)\n",
        "\n",
        "# Transform the data by scaling each feature to a range between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "np_data_scaled = scaler.fit_transform(np_data_unscaled)\n",
        "\n",
        "# Creating a separate scaler that works on a single column for scaling predictions\n",
        "scaler_pred = MinMaxScaler()\n",
        "df_Close = pd.DataFrame(data_filtered_ext['Close'])\n",
        "np_Close_scaled = scaler_pred.fit_transform(df_Close)"
      ],
      "metadata": {
        "id": "NWGhEQQlnXJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fda738d-d9b7-4c1b-e110-963a89dce2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1440, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the sequence length - this is the timeframe used to make a single prediction\n",
        "sequence_length = 100\n",
        "\n",
        "# Prediction Index\n",
        "index_Close = data.columns.get_loc(\"Close\")\n",
        "#index_Close = 1\n",
        "\n",
        "\n",
        "# Split the training data into train and train data sets\n",
        "# As a first step, we get the number of rows to train the model on 80% of the data \n",
        "train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n",
        "\n",
        "# Create the training and test data\n",
        "train_data = np_data_scaled[0:train_data_len, :]\n",
        "test_data = np_data_scaled[train_data_len - sequence_length:, :]\n",
        "\n",
        "# The RNN needs data with the format of [samples, time steps, features]\n",
        "# Here, we create N samples, sequence_length time steps per sample, and 6 features\n",
        "def partition_dataset(sequence_length, data):\n",
        "    x, y = [], []\n",
        "    data_len = data.shape[0]\n",
        "    for i in range(sequence_length, data_len):\n",
        "        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
        "        y.append(data[i, index_Close]) #contains the prediction values for validation,  for single-step prediction\n",
        "    \n",
        "    # Convert the x and y to numpy arrays\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return x, y\n",
        "\n",
        "# Generate training data and test data\n",
        "x_train, y_train = partition_dataset(sequence_length, train_data)\n",
        "x_test, y_test = partition_dataset(sequence_length, test_data)\n",
        "\n",
        "# Print the shapes: the result is: (rows, training_sequence, features) (prediction value, )\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n",
        "\n",
        "# Validate that the prediction value and the input match up\n",
        "# The last close price of the second input sample should equal the first prediction value\n",
        "print(x_train[1][sequence_length-1][index_Close])\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "OvUozCVKnfMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d021d30-7991-48d8-b5ae-c34e1efdde91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1052, 100, 5) (1052,)\n",
            "(288, 100, 5) (288,)\n",
            "0.01146016420088214\n",
            "0.01146016420088214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the neural network model\n",
        "model = Sequential()\n",
        "\n",
        "# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables\n",
        "n_neurons = x_train.shape[1] * x_train.shape[2]\n",
        "print(n_neurons, x_train.shape[1], x_train.shape[2])\n",
        "model.add(LSTM(n_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]))) \n",
        "model.add(LSTM(n_neurons, return_sequences=False))\n",
        "model.add(Dense(5))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "5BgUjF7pnpMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bdaae8-692b-4b26-d315-b96b987a6b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 100 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "history = model.fit(x_train, y_train, \n",
        "                    batch_size=batch_size, \n",
        "                    epochs=epochs,\n",
        "                    validation_data=(x_test, y_test)\n",
        "                   )\n",
        "                    \n",
        "                    #callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "9qPe69ltnuiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10098835-22aa-4263-d715-6edec6042d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "17/17 [==============================] - 6s 101ms/step - loss: 0.0376 - val_loss: 0.0092\n",
            "Epoch 2/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 3/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 9.8521e-04 - val_loss: 8.4158e-04\n",
            "Epoch 4/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 6.8447e-04 - val_loss: 5.6948e-04\n",
            "Epoch 5/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 5.6647e-04 - val_loss: 7.4884e-04\n",
            "Epoch 6/100\n",
            "17/17 [==============================] - 1s 64ms/step - loss: 4.1343e-04 - val_loss: 7.1343e-04\n",
            "Epoch 7/100\n",
            "17/17 [==============================] - 1s 63ms/step - loss: 4.4418e-04 - val_loss: 6.5211e-04\n",
            "Epoch 8/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 3.7686e-04 - val_loss: 5.7224e-04\n",
            "Epoch 9/100\n",
            "17/17 [==============================] - 1s 63ms/step - loss: 3.9783e-04 - val_loss: 6.0641e-04\n",
            "Epoch 10/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 3.6672e-04 - val_loss: 5.5066e-04\n",
            "Epoch 11/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 3.6139e-04 - val_loss: 5.5657e-04\n",
            "Epoch 12/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 3.5385e-04 - val_loss: 5.5443e-04\n",
            "Epoch 13/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 3.5358e-04 - val_loss: 7.0974e-04\n",
            "Epoch 14/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 3.4739e-04 - val_loss: 5.2772e-04\n",
            "Epoch 15/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 3.5214e-04 - val_loss: 5.5309e-04\n",
            "Epoch 16/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 4.0936e-04 - val_loss: 5.1837e-04\n",
            "Epoch 17/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 4.5156e-04 - val_loss: 8.2047e-04\n",
            "Epoch 18/100\n",
            "17/17 [==============================] - 1s 63ms/step - loss: 4.6691e-04 - val_loss: 5.2422e-04\n",
            "Epoch 19/100\n",
            "17/17 [==============================] - 1s 64ms/step - loss: 3.3684e-04 - val_loss: 6.2817e-04\n",
            "Epoch 20/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 3.6856e-04 - val_loss: 5.4533e-04\n",
            "Epoch 21/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 3.5042e-04 - val_loss: 5.3270e-04\n",
            "Epoch 22/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 3.6962e-04 - val_loss: 4.9278e-04\n",
            "Epoch 23/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 3.2716e-04 - val_loss: 4.8067e-04\n",
            "Epoch 24/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 3.1623e-04 - val_loss: 4.7672e-04\n",
            "Epoch 25/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 3.6345e-04 - val_loss: 7.3994e-04\n",
            "Epoch 26/100\n",
            "17/17 [==============================] - 1s 64ms/step - loss: 3.3826e-04 - val_loss: 4.7151e-04\n",
            "Epoch 27/100\n",
            "17/17 [==============================] - 1s 64ms/step - loss: 3.1689e-04 - val_loss: 4.7076e-04\n",
            "Epoch 28/100\n",
            "17/17 [==============================] - 1s 64ms/step - loss: 3.5304e-04 - val_loss: 4.7994e-04\n",
            "Epoch 29/100\n",
            "17/17 [==============================] - 1s 65ms/step - loss: 3.0828e-04 - val_loss: 4.6152e-04\n",
            "Epoch 30/100\n",
            "17/17 [==============================] - 1s 63ms/step - loss: 2.9653e-04 - val_loss: 4.8854e-04\n",
            "Epoch 31/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 3.0089e-04 - val_loss: 4.6950e-04\n",
            "Epoch 32/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.9207e-04 - val_loss: 4.4931e-04\n",
            "Epoch 33/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 3.1786e-04 - val_loss: 4.3685e-04\n",
            "Epoch 34/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 3.2094e-04 - val_loss: 5.3437e-04\n",
            "Epoch 35/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.8332e-04 - val_loss: 4.7928e-04\n",
            "Epoch 36/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.8732e-04 - val_loss: 4.3173e-04\n",
            "Epoch 37/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.7110e-04 - val_loss: 4.7394e-04\n",
            "Epoch 38/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.8346e-04 - val_loss: 6.0097e-04\n",
            "Epoch 39/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.9563e-04 - val_loss: 4.9893e-04\n",
            "Epoch 40/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.8977e-04 - val_loss: 4.1030e-04\n",
            "Epoch 41/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 2.9901e-04 - val_loss: 4.1659e-04\n",
            "Epoch 42/100\n",
            "17/17 [==============================] - 1s 65ms/step - loss: 2.8841e-04 - val_loss: 4.2535e-04\n",
            "Epoch 43/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.7027e-04 - val_loss: 4.8889e-04\n",
            "Epoch 44/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 3.4561e-04 - val_loss: 5.7605e-04\n",
            "Epoch 45/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.8831e-04 - val_loss: 4.1649e-04\n",
            "Epoch 46/100\n",
            "17/17 [==============================] - 1s 63ms/step - loss: 2.9507e-04 - val_loss: 5.3760e-04\n",
            "Epoch 47/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.7228e-04 - val_loss: 4.7014e-04\n",
            "Epoch 48/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.7710e-04 - val_loss: 3.8169e-04\n",
            "Epoch 49/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 2.7731e-04 - val_loss: 3.7167e-04\n",
            "Epoch 50/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.4939e-04 - val_loss: 3.7429e-04\n",
            "Epoch 51/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.9983e-04 - val_loss: 4.3180e-04\n",
            "Epoch 52/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 3.0898e-04 - val_loss: 4.8482e-04\n",
            "Epoch 53/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.6346e-04 - val_loss: 3.5237e-04\n",
            "Epoch 54/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 2.6210e-04 - val_loss: 5.2305e-04\n",
            "Epoch 55/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 2.9092e-04 - val_loss: 4.7502e-04\n",
            "Epoch 56/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 2.6391e-04 - val_loss: 3.4179e-04\n",
            "Epoch 57/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.5156e-04 - val_loss: 6.5796e-04\n",
            "Epoch 58/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.6538e-04 - val_loss: 3.3024e-04\n",
            "Epoch 59/100\n",
            "17/17 [==============================] - 1s 58ms/step - loss: 2.9517e-04 - val_loss: 3.3829e-04\n",
            "Epoch 60/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.4228e-04 - val_loss: 4.1431e-04\n",
            "Epoch 61/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.5099e-04 - val_loss: 4.5181e-04\n",
            "Epoch 62/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 2.2507e-04 - val_loss: 3.2218e-04\n",
            "Epoch 63/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.3242e-04 - val_loss: 3.4739e-04\n",
            "Epoch 64/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 2.8094e-04 - val_loss: 4.5162e-04\n",
            "Epoch 65/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.7929e-04 - val_loss: 4.0042e-04\n",
            "Epoch 66/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.4140e-04 - val_loss: 6.5409e-04\n",
            "Epoch 67/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.3755e-04 - val_loss: 2.9990e-04\n",
            "Epoch 68/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.2027e-04 - val_loss: 3.1255e-04\n",
            "Epoch 69/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.6519e-04 - val_loss: 3.2076e-04\n",
            "Epoch 70/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.6708e-04 - val_loss: 3.8684e-04\n",
            "Epoch 71/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 2.3316e-04 - val_loss: 3.6368e-04\n",
            "Epoch 72/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 2.2884e-04 - val_loss: 3.0124e-04\n",
            "Epoch 73/100\n",
            "17/17 [==============================] - 1s 58ms/step - loss: 2.5764e-04 - val_loss: 2.8649e-04\n",
            "Epoch 74/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.4715e-04 - val_loss: 4.0122e-04\n",
            "Epoch 75/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 2.6549e-04 - val_loss: 2.7413e-04\n",
            "Epoch 76/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.6723e-04 - val_loss: 3.2866e-04\n",
            "Epoch 77/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.0347e-04 - val_loss: 2.9107e-04\n",
            "Epoch 78/100\n",
            "17/17 [==============================] - 1s 62ms/step - loss: 2.3946e-04 - val_loss: 4.9119e-04\n",
            "Epoch 79/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 2.4215e-04 - val_loss: 4.1033e-04\n",
            "Epoch 80/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 2.4120e-04 - val_loss: 3.0125e-04\n",
            "Epoch 81/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.4093e-04 - val_loss: 4.6054e-04\n",
            "Epoch 82/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.3552e-04 - val_loss: 3.2077e-04\n",
            "Epoch 83/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.3295e-04 - val_loss: 2.7600e-04\n",
            "Epoch 84/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 1.9526e-04 - val_loss: 3.7006e-04\n",
            "Epoch 85/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.6059e-04 - val_loss: 3.1550e-04\n",
            "Epoch 86/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.3947e-04 - val_loss: 2.5038e-04\n",
            "Epoch 87/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.0181e-04 - val_loss: 2.7476e-04\n",
            "Epoch 88/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 1.9771e-04 - val_loss: 2.4751e-04\n",
            "Epoch 89/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 1.9643e-04 - val_loss: 2.6194e-04\n",
            "Epoch 90/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 2.1476e-04 - val_loss: 4.5313e-04\n",
            "Epoch 91/100\n",
            "17/17 [==============================] - 1s 66ms/step - loss: 2.9933e-04 - val_loss: 5.3030e-04\n",
            "Epoch 92/100\n",
            "17/17 [==============================] - 1s 61ms/step - loss: 3.4758e-04 - val_loss: 9.0032e-04\n",
            "Epoch 93/100\n",
            "17/17 [==============================] - 1s 63ms/step - loss: 3.6109e-04 - val_loss: 2.4345e-04\n",
            "Epoch 94/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.1673e-04 - val_loss: 3.5858e-04\n",
            "Epoch 95/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.8907e-04 - val_loss: 3.0854e-04\n",
            "Epoch 96/100\n",
            "17/17 [==============================] - 1s 59ms/step - loss: 2.1514e-04 - val_loss: 4.0465e-04\n",
            "Epoch 97/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 2.2655e-04 - val_loss: 2.3752e-04\n",
            "Epoch 98/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 1.7787e-04 - val_loss: 2.8306e-04\n",
            "Epoch 99/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 1.8688e-04 - val_loss: 3.5907e-04\n",
            "Epoch 100/100\n",
            "17/17 [==============================] - 1s 60ms/step - loss: 1.9508e-04 - val_loss: 5.1290e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss values\n",
        "fig, ax = plt.subplots(figsize=(16, 5), sharex=True)\n",
        "sns.lineplot(data=history.history[\"loss\"])\n",
        "plt.title(\"Model loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))\n",
        "plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b43Ck4wtn9y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted values\n",
        "y_pred_scaled = model.predict(x_test)\n",
        "\n",
        "# Unscale the predicted values\n",
        "y_pred = scaler_pred.inverse_transform(y_pred_scaled)\n",
        "y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Mean Absolute Error (MAE)\n",
        "MAE = mean_absolute_error(y_test_unscaled, y_pred)\n",
        "print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n",
        "\n",
        "# Mean Absolute Percentage Error (MAPE)\n",
        "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n",
        "\n",
        "# Median Absolute Percentage Error (MDAPE)\n",
        "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\n",
        "print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')"
      ],
      "metadata": {
        "id": "O-yWLbEhoDzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The date from which on the date is displayed\n",
        "display_start_date = \"2019-01-01\" \n",
        "\n",
        "# Add the difference between the valid and predicted prices\n",
        "train = pd.DataFrame(data_filtered_ext['Close'][:train_data_len + 1]).rename(columns={'Close': 'y_train'})\n",
        "valid = pd.DataFrame(data_filtered_ext['Close'][train_data_len:]).rename(columns={'Close': 'y_test'})\n",
        "valid.insert(1, \"y_pred\", y_pred, True)\n",
        "valid.insert(1, \"residuals\", valid[\"y_pred\"] - valid[\"y_test\"], True)\n",
        "df_union = pd.concat([train, valid])\n",
        "\n",
        "# Zoom in to a closer timeframe\n",
        "#df_union_zoom = df_union[df_union.index > display_start_date]\n",
        "\n",
        "# Create the lineplot\n",
        "fig, ax1 = plt.subplots(figsize=(16, 8))\n",
        "plt.title(\"y_pred vs y_test\")\n",
        "plt.ylabel(\"AAPL\", fontsize=18)\n",
        "sns.set_palette([\"#090364\", \"#1960EF\", \"#EF5919\"])\n",
        "sns.lineplot(data=df_union[['y_pred', 'y_train', 'y_test']], linewidth=1.0, dashes=False, ax=ax1)\n",
        "\n",
        "# Create the bar plot with the differences\n",
        "df_sub = [\"#2BC97A\" if x > 0 else \"#C92B2B\" for x in df_union[\"residuals\"].dropna()]\n",
        "ax1.bar(height=df_union['residuals'].dropna(), x=df_union['residuals'].dropna().index, width=3, label='residuals', color=df_sub)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CUFii8RWoIu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_pred"
      ],
      "metadata": {
        "id": "WaId5NAPtNaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "Jsim-Rg0tTMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Calculate RMSE performance metrics\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "math.sqrt(mean_squared_error(y_test_unscaled,y_pred))"
      ],
      "metadata": {
        "id": "R_3PKvuEjdja"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}